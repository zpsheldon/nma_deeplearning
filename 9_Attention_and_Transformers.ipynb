{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7915f283",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbe5ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tqdm\n",
    "import math\n",
    "import torch\n",
    "import random\n",
    "import statistics\n",
    "import requests\n",
    "import tarfile\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn\n",
    "from pprint import pprint\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import load_metric\n",
    "from datasets import load_dataset\n",
    "\n",
    "# transformers library\n",
    "from transformers import Trainer\n",
    "from transformers import pipeline\n",
    "from transformers import set_seed\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TrainingArguments\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# pytorch\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "from pytorch_pretrained_bert import BertForMaskedLM\n",
    "\n",
    "# textattack\n",
    "from textattack.transformations import WordSwapQWERTY\n",
    "from textattack.transformations import WordSwapExtend\n",
    "from textattack.transformations import WordSwapContract\n",
    "from textattack.transformations import WordSwapHomoglyphSwap\n",
    "from textattack.transformations import CompositeTransformation\n",
    "from textattack.transformations import WordSwapRandomCharacterDeletion\n",
    "from textattack.transformations import WordSwapNeighboringCharacterSwap\n",
    "from textattack.transformations import WordSwapRandomCharacterInsertion\n",
    "from textattack.transformations import WordSwapRandomCharacterSubstitution\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc9cb53",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aed263f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=None, seed_torch=True):\n",
    "    if seed is None:\n",
    "        seed = np.random.choice(2 ** 32)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if seed_torch:\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    print(f'Random seed {seed} has been set.')\n",
    "\n",
    "\n",
    "# In case that `DataLoader` is used\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "    \n",
    "def set_device():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    return device\n",
    "\n",
    "SEED = 2021\n",
    "set_seed(seed=SEED)\n",
    "DEVICE = set_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54153772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download dataset\n",
    "def load_yelp_data(DATASET, tokenizer):\n",
    "    dataset = DATASET\n",
    "    dataset['train'] = dataset['train'].select(range(10000))\n",
    "    dataset['test'] = dataset['test'].select(range(5000))\n",
    "    dataset = dataset.map(lambda e: tokenizer(e['text'], truncation=True,\n",
    "                                            padding='max_length'), batched=True)\n",
    "    dataset.set_format(type='torch', columns=['input_ids', 'label'])\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset['train'], batch_size=32)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset['test'], batch_size=32)\n",
    "\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    max_len = next(iter(train_loader))['input_ids'].shape[0]\n",
    "    num_classes = next(iter(train_loader))['label'].shape[0]\n",
    "\n",
    "    return train_loader, test_loader, max_len, vocab_size, num_classes\n",
    "\n",
    "\n",
    "url = \"https://osf.io/kthjg/download\"\n",
    "fname = \"huggingface.tar.gz\"\n",
    "\n",
    "if not os.path.exists(fname):\n",
    "    print('Dataset is downloading...')\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    with open(fname, 'wb') as fd:\n",
    "        fd.write(r.content)\n",
    "    print('Download is finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25683ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = load_dataset(\"yelp_review_full\", download_mode=\"reuse_dataset_if_exists\")\n",
    "print(type(DATASET))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "train_loader, test_loader, max_len, vocab_size, num_classes = load_yelp_data(DATASET, tokenizer)\n",
    "\n",
    "pred_text    = DATASET['test']['text'][28]\n",
    "actual_label = DATASET['test']['label'][28]\n",
    "batch1 = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15cfef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_sentence_for_bert(sent, masked_word = \"___\"):\n",
    "    \"\"\"\n",
    "    By default takes a sentence with ___ instead of a masked word.\n",
    "\n",
    "    Args:\n",
    "    sent (str): an input sentence\n",
    "    masked_word(str): a masked part of the sentence\n",
    "\n",
    "    Returns:\n",
    "    str: sentence that could be bassed to BERT\n",
    "    \"\"\"\n",
    "    splitted = sent.split(\"___\")\n",
    "    assert (len(splitted) == 2), \"Missing masked word. Make sure to mark it as ___\"\n",
    "\n",
    "    return '[CLS] ' + splitted[0] + \"[MASK]\" + splitted[1] + ' [SEP]'\n",
    "\n",
    "\n",
    "def parse_text_and_words(raw_line, mask = \"___\"):\n",
    "    \"\"\"\n",
    "    Takes a line that has multiple options for some position in the text.\n",
    "\n",
    "    Input: The doctor picked up his/her bag\n",
    "    Output: (The doctor picked up ___ bag, ['his', 'her'])\n",
    "\n",
    "    Args:\n",
    "    raw_line (str): a line in format 'some text option1/.../optionN some text'\n",
    "    mask (str): the replacement for .../... section\n",
    "    Returns:\n",
    "    str: text with mask instead of .../... section\n",
    "    list: list of words from the .../... section\n",
    "    \"\"\"\n",
    "    splitted = raw_line.split(' ')\n",
    "    mask_index = -1\n",
    "    for i in range(len(splitted)):\n",
    "        if \"/\" in splitted[i]:\n",
    "            mask_index = i\n",
    "            break\n",
    "    assert(mask_index != -1), \"No '/'-separated words\"\n",
    "    words = splitted[mask_index].split('/')\n",
    "    splitted[mask_index] = mask\n",
    "    return \" \".join(splitted), words\n",
    "\n",
    "\n",
    "def get_probabilities_of_masked_words(text, words):\n",
    "    \"\"\"\n",
    "    Computes probabilities of each word in the masked section of the text.\n",
    "    Args:\n",
    "    text (str): A sentence with ___ instead of a masked word.\n",
    "    words (list): array of words.\n",
    "    Returns:\n",
    "    list: predicted probabilities for given words.\n",
    "    \"\"\"\n",
    "    text = transform_sentence_for_bert(text)\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    for i in range(len(words)):\n",
    "        words[i] = tokenizer.tokenize(words[i])[0]\n",
    "    words_idx = [tokenizer.convert_tokens_to_ids([word]) for word in words]\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    masked_index = tokenized_text.index('[MASK]')\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "\n",
    "    pretrained_masked_model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "    pretrained_masked_model.eval()\n",
    "\n",
    "    # Predict all tokens\n",
    "    with torch.no_grad():\n",
    "        predictions = pretrained_masked_model(tokens_tensor)\n",
    "    probabilities = F.softmax(predictions[0][masked_index], dim = 0)\n",
    "    predicted_index = torch.argmax(probabilities).item()\n",
    "\n",
    "    return [probabilities[ix].item() for ix in words_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249ffe26",
   "metadata": {},
   "source": [
    "# Attention overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef420f6",
   "metadata": {},
   "source": [
    "## Queries, Keys, & Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff60dc9",
   "metadata": {},
   "source": [
    "One way to think about attention is to consider a dictionary that contains all information needed for our task. Each entry in the dictionary contains some value and the corresponding key to retrieve it. For a specific prediction, we would like to retrieve relevant information from the dictionary. Therefore, we issue a query, match it to keys in the dictionary, and return the corresponding values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415e2165",
   "metadata": {},
   "source": [
    "Let's compute the scaled dot product attention using its matrix form. \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{softmax} \\left( \\frac{Q K^\\text{T}}{\\sqrt{d}} \\right) V\n",
    "\\end{equation}\n",
    "\n",
    "where $Q$ denotes the query or values of the embeddings (in other words the hidden states), $K$ the key, and $k$ denotes the dimension of the query key vector.\n",
    "\n",
    "Note: the function takes an additional argument `h` (number of heads). You can assume it is 1 for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b9e96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProductAttention(nn.Module):\n",
    "    \"\"\"Scaled dot product attention.\"\"\"\n",
    "    def __init__(self, dropout, **kwargs):\n",
    "        super(DotProductAttention, self).__init__(**kwargs)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, queries, keys, values, b, h, t, k):\n",
    "        \"\"\"\n",
    "        Compute dot products. This is the same operation for each head,\n",
    "        so we can fold the heads into the batch dimension and use torch.bmm\n",
    "        Note: .contiguous() doesn't change the actual shape of the data,\n",
    "        but it rearranges the tensor in memory, which will help speed up the computation\n",
    "        for this batch matrix multiplication.\n",
    "        .transpose() is used to change the shape of a tensor. It returns a new tensor\n",
    "        that shares the data with the original tensor. It can only swap two dimension.\n",
    "\n",
    "        Shape of `queries`: (`batch_size`, no. of queries, head,`k`)\n",
    "        Shape of `keys`: (`batch_size`, no. of key-value pairs, head, `k`)\n",
    "        Shape of `values`: (`batch_size`, no. of key-value pairs, head, value dimension)\n",
    "\n",
    "        b: batch size\n",
    "        h: number of heads\n",
    "        t: number of keys/queries/values (for simplicity, let's assume they have the same sizes)\n",
    "        k: embedding size\n",
    "        \"\"\"\n",
    "        keys = keys.transpose(1, 2).contiguous().view(b * h, t, k)\n",
    "        queries = queries.transpose(1, 2).contiguous().view(b * h, t, k)\n",
    "        values = values.transpose(1, 2).contiguous().view(b * h, t, k)\n",
    "\n",
    "        # Matrix Multiplication between the keys and queries\n",
    "        score = torch.bmm(queries, keys.transpose(1, 2)) / math.sqrt(k)  # size: (b * h, t, t)\n",
    "        softmax_weights = F.softmax(score, dim=2)  # row-wise normalization of weights\n",
    "\n",
    "        # Matrix Multiplication between the output of the key and queries multiplication and values.\n",
    "        out = torch.bmm(self.dropout(softmax_weights), values).view(b, h, t, k)  # rearrange h and t dims\n",
    "        out = out.transpose(1, 2).contiguous().view(b, t, h * k)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2f6f84",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4c2dd7",
   "metadata": {},
   "source": [
    "A transformer block consists of three core layers (on top of the input): self attention, layer normalization, and feedforward neural network.\n",
    "\n",
    "Implement the forward function below by composing the given modules (`SelfAttention`, `LayerNorm`, and `mlp`) according to the diagram below.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W2D4_AttentionAndTransformers/static/transformers1.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f184e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Transformer Block\n",
    "    Args:\n",
    "    k (int): Attention embedding size\n",
    "    heads (int): number of self-attention heads\n",
    "\n",
    "    Attributes:\n",
    "    attention: Multi-head SelfAttention layer\n",
    "    norm_1, norm_2: LayerNorms\n",
    "    mlp: feedforward neural network\n",
    "    \"\"\"\n",
    "    def __init__(self, k, heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = SelfAttention(k, heads=heads)\n",
    "\n",
    "        self.norm_1 = nn.LayerNorm(k)\n",
    "        self.norm_2 = nn.LayerNorm(k)\n",
    "\n",
    "        hidden_size = 2 * k  # This is a somewhat arbitrary choice\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(k, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, k))\n",
    "\n",
    "    def forward(self, x):\n",
    "        attended = self.attention(x)\n",
    "        # Complete the input of the first Add & Normalize layer\n",
    "        x = self.norm_1(attended + x)\n",
    "\n",
    "        feedforward = self.mlp(x)\n",
    "        # Complete the input of the second Add & Normalize layer\n",
    "        x = self.norm_2(feedforward + x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fa49ff",
   "metadata": {},
   "source": [
    "# Multi-head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22594627",
   "metadata": {},
   "source": [
    "One powerful idea in Transformer is multi-head attention, which is used to capture different aspects of the dependence among words (e.g., syntactical vs semantic). For more info see [here](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#a-family-of-attention-mechanisms)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e95564",
   "metadata": {},
   "source": [
    "In self-attention, the queries, keys, and values are all mapped (by linear projection) from the word embeddings. Implement the mapping functions (`to_keys`, `to_queries`, `to_values`) below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1be39f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"Multi-head self attention layer\n",
    "\n",
    "    Args:\n",
    "    k (int): Size of attention embeddings\n",
    "    heads (int): Number of attention heads\n",
    "\n",
    "    Attributes:\n",
    "    to_keys: Transforms input to k x k*heads key vectors\n",
    "    to_queries: Transforms input to k x k*heads query vectors\n",
    "    to_values: Transforms input to k x k*heads value vectors\n",
    "    unify_heads: combines queries, keys and values to a single vector\n",
    "    \"\"\"\n",
    "    def __init__(self, k, heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.k, self.heads = k, heads\n",
    "        self.to_keys = nn.Linear(k, k * heads, bias=False)\n",
    "        self.to_queries = nn.Linear(k, k * heads, bias=False)\n",
    "        self.to_values = nn.Linear(k, k * heads, bias=False)\n",
    "        self.unify_heads = nn.Linear(k * heads, k)\n",
    "\n",
    "        self.attention = DotProductAttention(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Implements forward pass of self-attention layer\n",
    "\n",
    "        Args:\n",
    "          x (torch.Tensor): batch x t x k sized input\n",
    "        \"\"\"\n",
    "        b, t, k = x.size()\n",
    "        h = self.heads\n",
    "\n",
    "        # We reshape the queries, keys and values so that each head has its own dimension\n",
    "        queries = self.to_queries(x).view(b, t, h, k)\n",
    "        keys = self.to_keys(x).view(b, t, h, k)\n",
    "        values = self.to_values(x).view(b, t, h, k)\n",
    "\n",
    "        out = self.attention(queries, keys, values, b, h, t, k)\n",
    "\n",
    "        return self.unify_heads(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db91622a",
   "metadata": {},
   "source": [
    "# Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3cab2a",
   "metadata": {},
   "source": [
    "Self-attention is not sensitive to positions or word orderings. Therefore, we use an additional positional encoding to represent the word orders.\n",
    "\n",
    "There are multiple ways to encode the position. For our purpose to have continuous values of the positions based on binary encoding, let's use the following implementation of deterministic (as opposed to learned) position encoding using sinusoidal functions.\n",
    "\n",
    "Note that in the `forward` function, the positional embedding (`pe`) is added to the token embeddings (`x`) elementwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4697fa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    # Source: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "    def __init__(self, emb_size, dropout=0.1, max_len=512):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, emb_size)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, emb_size, 2).float() * (-np.log(10000.0) / emb_size))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184dbcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"Transformer Encoder network for classification\n",
    "\n",
    "    Args:\n",
    "      k (int): Attention embedding size\n",
    "      heads (int): Number of self attention heads\n",
    "      depth (int): How many transformer blocks to include\n",
    "      seq_length (int): How long an input sequence is\n",
    "      num_tokens (int): Size of dictionary\n",
    "      num_classes (int): Number of output classes\n",
    "    \"\"\"\n",
    "    def __init__(self, k, heads, depth, seq_length, num_tokens, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.k = k\n",
    "        self.num_tokens = num_tokens\n",
    "        self.token_embedding = nn.Embedding(num_tokens, k)\n",
    "        self.pos_enc = PositionalEncoding(k)\n",
    "\n",
    "        transformer_blocks = []\n",
    "        for i in range(depth):\n",
    "            transformer_blocks.append(TransformerBlock(k=k, heads=heads))\n",
    "\n",
    "        self.transformer_blocks = nn.Sequential(*transformer_blocks)\n",
    "        self.classification_head = nn.Linear(k, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass for Classification Transformer network\n",
    "\n",
    "        Args:\n",
    "          x (torch.Tensor): (b, t) sized tensor of tokenized words\n",
    "\n",
    "        Returns:\n",
    "          torch.Tensor of size (b, c) with log-probabilities over classes\n",
    "        \"\"\"\n",
    "        x = self.token_embedding(x) * np.sqrt(self.k)\n",
    "        x = self.pos_enc(x)\n",
    "        x = self.transformer_blocks(x)\n",
    "        sequence_avg = x.mean(dim=1)\n",
    "        x = self.classification_head(sequence_avg)\n",
    "        logprobs = F.log_softmax(x, dim=1)\n",
    "\n",
    "        return logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbdbf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loss_fn, train_loader,\n",
    "          n_iter=1, learning_rate=1e-4,\n",
    "          test_loader=None, device='cpu',\n",
    "          L2_penalty=0, L1_penalty=0):\n",
    "    \"\"\"Run gradient descent to opimize parameters of a given network\n",
    "\n",
    "    Args:\n",
    "    net (nn.Module): PyTorch network whose parameters to optimize\n",
    "    loss_fn: built-in PyTorch loss function to minimize\n",
    "    train_data (torch.Tensor): n_train x n_neurons tensor with neural\n",
    "      responses to train on\n",
    "    train_labels (torch.Tensor): n_train x 1 tensor with orientations of the\n",
    "      stimuli corresponding to each row of train_data\n",
    "    n_iter (int, optional): number of iterations of gradient descent to run\n",
    "    learning_rate (float, optional): learning rate to use for gradient descent\n",
    "    test_data (torch.Tensor, optional): n_test x n_neurons tensor with neural\n",
    "      responses to test on\n",
    "    test_labels (torch.Tensor, optional): n_test x 1 tensor with orientations of\n",
    "      the stimuli corresponding to each row of test_data\n",
    "    L2_penalty (float, optional): l2 penalty regularizer coefficient\n",
    "    L1_penalty (float, optional): l1 penalty regularizer coefficient\n",
    "\n",
    "    Returns:\n",
    "    (list): training loss over iterations\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize PyTorch Adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Placeholder to save the loss at each iteration\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "\n",
    "    # Loop over epochs (cf. appendix)\n",
    "    for iter in range(n_iter):\n",
    "        iter_train_loss = []\n",
    "        for i, batch in tqdm(enumerate(train_loader)):\n",
    "            # compute network output from inputs in train_data\n",
    "            out = model(batch['input_ids'].to(device))\n",
    "            loss = loss_fn(out, batch['label'].to(device))\n",
    "\n",
    "            # Clear previous gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Compute gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # Store current value of loss\n",
    "            iter_train_loss.append(loss.item())  # .item() needed to transform the tensor output of loss_fn to a scalar\n",
    "            if i % 50 == 0:\n",
    "                print(f'[Batch {i}]: train_loss: {loss.item()}')\n",
    "        train_loss.append(statistics.mean(iter_train_loss))\n",
    "\n",
    "        # Track progress\n",
    "        if True: #(iter + 1) % (n_iter // 5) == 0:\n",
    "\n",
    "            if test_loader is not None:\n",
    "                print('Running Test loop')\n",
    "            iter_loss_test = []\n",
    "            for j, test_batch in enumerate(test_loader):\n",
    "\n",
    "                out_test = model(test_batch['input_ids'].to(device))\n",
    "                loss_test = loss_fn(out_test, test_batch['label'].to(device))\n",
    "                iter_loss_test.append(loss_test.item())\n",
    "\n",
    "            test_loss.append(statistics.mean(iter_loss_test))\n",
    "\n",
    "            if test_loader is None:\n",
    "                print(f'iteration {iter + 1}/{n_iter} | train loss: {loss.item():.3f}')\n",
    "            else:\n",
    "                print(f'iteration {iter + 1}/{n_iter} | train loss: {loss.item():.3f} | test_loss: {loss_test.item():.3f}')\n",
    "\n",
    "    if test_loader is None:\n",
    "        return train_loss\n",
    "    else:\n",
    "        return train_loss, test_loss\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize network with embedding size 128, 8 attention heads, and 3 layers\n",
    "model = Transformer(128, 8, 3, max_len, vocab_size, num_classes).to(device)\n",
    "\n",
    "# Initialize built-in PyTorch Negative Log Likelihood loss function\n",
    "loss_fn = F.nll_loss\n",
    "\n",
    "train_loss, test_loss = train(model, loss_fn, train_loader, test_loader=test_loader,\n",
    "                              device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48480cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # Batch 1 contains all the tokenized text for the 1st batch of the test loader\n",
    "    pred_batch = model(batch1['input_ids'].to(device))\n",
    "    # Predicting the label for the text\n",
    "    print(\"The yelp review is → \" + str(pred_text))\n",
    "    predicted_label28 = np.argmax(pred_batch[28].cpu())\n",
    "    print()\n",
    "    print(\"The Predicted Rating is → \" + str(predicted_label28) + \" and the Actual Rating was → \" + str(actual_label))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nma",
   "language": "python",
   "name": "nma"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
